# üöÄ DenseCore Performance Benchmark Report

**Last Updated**: 2025-12-20
**Platform**: Intel Core i7-10870H (Comet Lake, 8C/16T)
**Quantization**: Q4_K_M (INT4)
**SIMD**: AVX2 + FMA3 (Runtime CPUID detection - portable binary)

---

## üìä Latest Benchmark Results

Tested on Intel Comet Lake CPU with AVX2 optimized kernels.

| Model | Size | Load Time | **TPS** | Context |
|-------|------|-----------|---------|---------| 
| **Qwen2.5-0.5B** | 0.5 GB | 1.2s | **Hang (Issue #1)** | 32768 |
| **TinyLlama-1.1B** | 0.7 GB | 6.9s | **22.1*** | 4096 |
| **Qwen3-4B** | 2.5 GB | 3.5s | **Hang (Issue #1)** | 40960 |
| **Llama-3-8B** | 5.2 GB | 28.4s | **Hang (Issue #1)** | 8192 |
| **Qwen3-8B** | 4.7 GB | 25.1s | **Hang (Issue #1)** | 32768 |

> *TinyLlama result verified in previous stable build.

> ‚ö†Ô∏è **Verification Update (2025-12-20):**
> DenseCore Engine v0.2.0 loads and initializes all Qwen models correctly (verified by logs).
> 
> **AVX2 Hardening Applied:**
> - Added explicit barrier safety guards to `ComputeQKV_AVX2` and `ComputeQKV_Scalar`
> - Added zero-work thread early exit checks (`start_col >= end_col`)
> - Edge case unit tests pass (32/32 SimdOps tests)
>
> **Current Issue (Issue #1):**
> - Hangs occur at first token generation (after model load completes)
> - Root cause: Suspected GGML graph compute or KV cache write operation
> - Workaround: Use 1 thread (`--threads 1`) or AVX-512 hardware

---

## üÜö DenseCore vs HuggingFace Transformers

| Model | DenseCore TPS | Transformers TPS | **Speedup** |
|-------|---------------|-----------------|-------------|
| Qwen2.5-0.5B | **-** | ~3-4 | **-** |
| TinyLlama-1.1B | **22.1** | ~2 | **11x** |
| Llama-3-8B | **-** | ~0.8 | **-** |
| Qwen3-8B | **-** | ~0.5 | **-** |

> Note: Transformers benchmarks run on same hardware with standard FP32/FP16 execution.

---

## ‚òÅÔ∏è AWS Instance Cost Analysis

**Scenario:** deploy Qwen2.5-0.5B for high-throughput app.

| Instance | vCPU | Cost/hr | TPS | Cost per 1M tok |
|----------|------|---------|-----|-----------------|
| **DenseCore (c7i.large)** | 2 | $0.085 | ~28* | **$0.84** |
| **GPU (g4dn.xlarge)** | 4 | $0.526 | ~50 | $2.92 |

> *Estimated based on architectural targets.

> üí∞ **Savings:** DenseCore is targetting **3.5x cheaper** per token generated compared to GPU instances for SLMs.

---

## üìà Performance by Model Size

```
Small Models (0.5-1B):
  ‚îú‚îÄ Qwen2.5-0.5B: TBD (Unstable)
  ‚îî‚îÄ TinyLlama-1.1B: 22.1 tok/s ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà

Medium Models (4-8B):
  ‚îú‚îÄ Qwen3-4B: TBD (Hang)
  ‚îú‚îÄ Llama-3-8B: TBD (Hang)
  ‚îî‚îÄ Qwen3-8B: TBD (Hang)
```

---

## üîß Optimization Details

1.  **Graph Caching**: Reuses computation graphs, saving 30% CPU cycles on small batch sizes.
2.  **Continuous Batching**: Maximizes CPU utilization by processing requests immediately.
3.  **SIMD Kernels**: Runtime CPUID-based dispatch (portable binaries):
    - **AVX-512**: Full 512-bit vectors (16 floats), 16x register blocking for GEMM
    - **AVX2 + FMA3**: 256-bit vectors (8 floats), 8x register blocking for GEMM
    - **Scalar**: Fallback for legacy CPUs
    - *Binary compiled with AVX-512 will auto-fallback to AVX2 on Comet Lake/Skylake*
4.  **INT4 Unpacking**: Optimized nibble extraction with shift-based sign extension (no branches).

---

## ‚úÖ Tested Architectures

| Architecture | Models | Status |
|--------------|--------|--------|
| **qwen2** | Qwen2.5-0.5B, Qwen2.5-1.5B | ‚ö†Ô∏è Unstable (AVX2) |
| **qwen3** | Qwen3-4B, Qwen3-8B | ‚ö†Ô∏è Unstable (AVX2) |
| **llama** | TinyLlama-1.1B, Llama-3.2 | ‚úÖ Verified |
| **phi3** | Phi-3.5-Mini | ‚ö†Ô∏è In Progress (Q8_0 verified, Q4 pending) |

---

## üéØ Use Case Recommendations

| Use Case | Recommended Model | Expected TPS |
|----------|-------------------|--------------|
| **Real-time Chat** | Qwen2.5-0.5B | 25+ tok/s (Pending Fix) |
| **function_calling** | TinyLlama-1.1B | 20+ tok/s |
| **RAG / Analytics** | Qwen3-8B | 6+ tok/s (Pending Fix) |

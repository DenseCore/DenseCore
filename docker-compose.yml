services:
  # ===========================================
  # DenseCore LLM Inference Server
  # ===========================================
  densecore:
    build:
      context: .
      dockerfile: Dockerfile
    image: densecore/densecore:latest # Official Docker Hub image
    container_name: densecore-server
    ports:
      - "8080:8080"
    volumes:
      # Mount your models directory (change the path as needed)
      - ./models:/models:ro
    environment:
      # Model configuration
      - MAIN_MODEL_PATH=/models/model.gguf # Change to your model filename
      # Server configuration
      - PORT=8080
      - HOST=0.0.0.0
      - THREADS=0 # 0 = auto-detect
      # Rate limiting
      - RATE_LIMIT_ENABLED=true
      - RATE_LIMIT_RPS=100
      - RATE_LIMIT_BURST=200
      # Logging
      - LOG_FORMAT=json
      # Timeouts
      - READ_TIMEOUT=30s
      - WRITE_TIMEOUT=120s
      - SHUTDOWN_TIMEOUT=30s
      # CORS
      - CORS_ALLOWED_ORIGINS=*
      # Authentication (set to true for production)
      - AUTH_ENABLED=false
      # - API_KEYS=sk-prod-key:user1:enterprise,sk-dev-key:user2:free
    healthcheck:
      test: [ "CMD", "wget", "-q", "--spider", "http://localhost:8080/health/live" ]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    restart: unless-stopped
    networks:
      - densecore-network
    deploy:
      resources:
        limits:
          cpus: '4'
          memory: 8G
        reservations:
          cpus: '2'
          memory: 4G

networks:
  densecore-network:
    driver: bridge
